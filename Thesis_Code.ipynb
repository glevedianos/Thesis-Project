{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PnmiUR1P0AN"
      },
      "source": [
        "#SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DHOjMw03JHf1"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/Diplo2\"\n",
        "%env PYTHONPATH =\n",
        "!pip install virtualenv\n",
        "!virtualenv semgs\n",
        "#!wget https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y --prefix /usr/local python=3.6 ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5gsqF33nJXjj"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/Diplo2\"\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "import os\n",
        "os.environ['CONDA_PREFIX'] = '/content/drive/MyDrive/Diplo2/semgs'\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install keras==2.2.4\n",
        "!pip install numpy==1.16.2\n",
        "!pip install scipy==1.2.1\n",
        "!pip install math\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn==0.20.3\n",
        "!pip install virtualenv\n",
        "!virtualenv semgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGstXv7bYvSY"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpJ_N9bwP7B-"
      },
      "source": [
        "#separate semg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ep5yWjvEW3Ik"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for s in range (1,28):\n",
        "    DATA = []\n",
        "    gesture_number = 0\n",
        "    for file in sorted(glob.glob(\"/content/drive/MyDrive/Diplo2/Ninapro/s{}/*.mat\".format(s))):\n",
        "        data = loadmat(file)\n",
        "        emg = data['emg']\n",
        "        restimulus = data['restimulus']\n",
        "        rerepetition = data['rerepetition']\n",
        "\n",
        "        print(\"EMG size {}\".format(emg.shape))\n",
        "\n",
        "        plt.subplot(3,1,1)\n",
        "        plt.plot(emg)\n",
        "        plt.subplot(3,1,2)\n",
        "        plt.plot(restimulus)\n",
        "        plt.subplot(3,1,3)\n",
        "        plt.plot(rerepetition)\n",
        "        plt.show()\n",
        "\n",
        "        L = len(restimulus)\n",
        "\n",
        "        previous_gesture = 0\n",
        "        current_signal = np.array([]).astype(np.float32)\n",
        "\n",
        "        for i in range(L):\n",
        "            current_gesture = restimulus[i]\n",
        "\n",
        "            if current_gesture != 0:\n",
        "                if previous_gesture == 0:\n",
        "                    gesture_number += (rerepetition[i].item() == 1)\n",
        "                    current_signal = np.append(current_signal, emg[i])\n",
        "                else:\n",
        "                    current_signal = np.vstack((current_signal, emg[i]))\n",
        "\n",
        "            elif (current_gesture == 0 and previous_gesture != 0) or (i == L - 1 and current_gesture != 0):\n",
        "                DATA.append({\n",
        "                    \"emg\": current_signal,\n",
        "                    \"repetition\": rerepetition[i-1].item(),\n",
        "                    \"stimulus\": gesture_number\n",
        "                })\n",
        "                # name = getSubjectKey(sub, gesture_number, rerepetition[i - 1].item())\n",
        "                # save_signal = {\n",
        "                #     \"emg\": current_signal,\n",
        "                # }\n",
        "\n",
        "                # if rerepetition[i-1] != 2 and rerepetition[i-1] != 5 and rerepetition[i-1] != 7:\n",
        "                #     savemat(folder_3 + \"/S{}_G{}_R{}.mat\".format(sub, gesture_number, rerepetition[i - 1].item()), save_signal)\n",
        "                # else:\n",
        "                #     savemat(folder_4 + \"/S{}_G{}_R{}.mat\".format(sub, gesture_number, rerepetition[i - 1].item()), save_signal)\n",
        "\n",
        "                current_signal = np.array([]).astype(np.float32)\n",
        "            previous_gesture = current_gesture\n",
        "    DATA_TRAIN = [x for x in DATA if x[\"repetition\"] not in [2,5,7]]\n",
        "    DATA_TEST = [x for x in DATA if x[\"repetition\"] in [2,5,7]]\n",
        "    np.save(\"/content/drive/MyDrive/Diplo2/Data/DATA_TRAIN{}\".format(s),DATA_TRAIN)\n",
        "    np.save(\"/content/drive/MyDrive/Diplo2/Data/DATA_TEST{}\".format(s),DATA_TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMRu7WS2QEjC"
      },
      "source": [
        "#Select Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5njwcaN0arWJ"
      },
      "outputs": [],
      "source": [
        "s=1\n",
        "DATA_TRAIN=np.load(\"/content/drive/MyDrive/Diplo2/Data/DATA_TRAIN{}.npy\".format(s), allow_pickle=True)\n",
        "DATA_TEST=np.load(\"/content/drive/MyDrive/Diplo2/Data/DATA_TEST{}.npy\".format(s), allow_pickle=True)\n",
        "print(len(DATA_TRAIN))\n",
        "print(len(DATA_TEST))\n",
        "print(DATA_TRAIN.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAS8jKy6QHOf"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz3JwZJ0bo0G"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 15\n",
        "WINDOW_SLIDE = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTQmfmXe3LaH"
      },
      "outputs": [],
      "source": [
        "import scipy.signal\n",
        "import math\n",
        "def lpf(x):\n",
        "    f = 1 / 50\n",
        "    b, a = scipy.signal.butter(1, f, 'low')\n",
        "    output = scipy.signal.filtfilt(b, a, x, axis=0)\n",
        "    return output\n",
        "\n",
        "def noise(x, snr_l=25):\n",
        "    snr_u = 45\n",
        "    db = np.random.randint(snr_l, snr_u, (1,))[0]\n",
        "    snr = 10 ** (db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0) / x.shape[0]\n",
        "    Np = Xp / snr\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np))\n",
        "    xn = x + n\n",
        "    return xn\n",
        "\n",
        "def mlaw(x):\n",
        "  x = (x - np.min(x)) / (np.max(x) - np.min(x))*2 - 1\n",
        "  m = 32\n",
        "  # Apply the mlaw logic element-wise\n",
        "  mlaw_func = np.vectorize(lambda val: (math.log(1 + m * abs(val)) / math.log(1 + m)) * math.copysign(1, val))\n",
        "  return mlaw_func(x)\n",
        "\n",
        "l=180\n",
        "for x in DATA_TRAIN:\n",
        "    emg = x[\"emg\"]\n",
        "    if l > len(emg):\n",
        "        l = len(emg)\n",
        "for x in DATA_TEST:\n",
        "    emg = x[\"emg\"]\n",
        "    if l > len(emg):\n",
        "        l = len(emg)\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FYtaAM9Wjp-P"
      },
      "outputs": [],
      "source": [
        "emg = DATA_TRAIN[86][\"emg\"]\n",
        "plt.plot(emg)\n",
        "plt.show()\n",
        "print(DATA_TRAIN[86][\"stimulus\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YT5KeiLY1PIT"
      },
      "outputs": [],
      "source": [
        "plt.plot((emg - np.min(emg)) / (np.max(emg) - np.min(emg)))\n",
        "emg= (emg - np.min(emg)) / (np.max(emg) - np.min(emg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TDFL_5335M6P"
      },
      "outputs": [],
      "source": [
        "\n",
        "emg = np.array(emg)\n",
        "print(emg.shape)\n",
        "cut = int((len(emg)-l)/2)\n",
        "for i in range(cut):\n",
        "    emg=np.delete(emg, 0, 0)\n",
        "for i in range (len(emg)-l):\n",
        "    emg=np.delete(emg, l, 0)\n",
        "plt.plot(emg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UeWkilJ81CHs"
      },
      "outputs": [],
      "source": [
        "emg=lpf(emg)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WfXjSEPsf4DM"
      },
      "outputs": [],
      "source": [
        "emg=mlaw(emg)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fJNmP2WE1wJj"
      },
      "outputs": [],
      "source": [
        "emg = noise(emg)\n",
        "print (emg.shape)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ0Gh19rb2sL"
      },
      "outputs": [],
      "source": [
        "import scipy.signal\n",
        "X_train, y_train = [], []\n",
        "def lpf(x):\n",
        "    f = 1 / 50\n",
        "    b, a = scipy.signal.butter(1, f, 'low')\n",
        "    output = scipy.signal.filtfilt(b, a, x, axis=0)\n",
        "    return output\n",
        "\n",
        "def noise(x, snr_l=25):\n",
        "    snr_u = 45\n",
        "    db = np.random.randint(snr_l, snr_u, (1,))[0]\n",
        "    snr = 10 ** (db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0) / x.shape[0]\n",
        "    Np = Xp / snr\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np))\n",
        "    xn = x + n\n",
        "    return xn\n",
        "\n",
        "def mlaw(x):\n",
        "  m = 32\n",
        "  if m==0: return x\n",
        "\n",
        "\n",
        "  #x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "  # Apply the mlaw logic element-wise\n",
        "  mlaw_func = np.vectorize(lambda val: (math.log(1 + m * abs(val)) / math.log(1 + m)) * math.copysign(1, val))\n",
        "  return mlaw_func(x)\n",
        "\n",
        "\n",
        "aug = 4\n",
        "\n",
        "z=0\n",
        "for j in range (aug):\n",
        "    for x in DATA_TRAIN:\n",
        "\n",
        "        emg = x[\"emg\"]\n",
        "        if(len(emg)>l):\n",
        "          z=z+1\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "        emg = lpf(emg)\n",
        "        emg = mlaw(emg)\n",
        "        emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "        emg = noise(emg)\n",
        "        for i in range(0, len(emg)-WINDOW_SIZE, WINDOW_SLIDE):\n",
        "            X_train.append(emg[i:i+WINDOW_SIZE, :])\n",
        "            y_train.append(x[\"stimulus\"])\n",
        "print(z)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NSpLr_qeb4sB"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = [], []\n",
        "\n",
        "for x in DATA_TEST:\n",
        "    emg = x[\"emg\"]\n",
        "    if(len(emg)>l):\n",
        "          z=z+1\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "    emg = lpf(emg)\n",
        "    emg = mlaw(emg)\n",
        "    emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "    emg = noise(emg)\n",
        "    for i in range(0, len(emg)-WINDOW_SIZE, WINDOW_SLIDE):\n",
        "        X_test.append(emg[i:i+WINDOW_SIZE, :])\n",
        "        y_test.append(x[\"stimulus\"])\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WYngaDZpb_IC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(np.unique(y_train))\n",
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMIdV07EcG0D"
      },
      "outputs": [],
      "source": [
        "!source /content/drive/MyDrive/Diplo2/semgs/bin/activate\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Labels 1-52 should be mapped to 0-51 and then encoded\n",
        "y_train = to_categorical(le.transform(y_train), 52)\n",
        "y_test = to_categorical(le.transform(y_test), 52)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRuGz-JlcHmN"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nweCywXyhDB5"
      },
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyBVV7N19kZQ"
      },
      "outputs": [],
      "source": [
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qTC8cuEzhDHt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "indices = np.random.choice(len(X_train), len(X_train), False)\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "indices = np.random.choice(len(X_test), len(X_test), False)\n",
        "X_test = X_test[indices]\n",
        "y_test = y_test[indices]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKjaCKZacQEN"
      },
      "outputs": [],
      "source": [
        "from keras.metrics import *\n",
        "\n",
        "def top3acc(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "def top5acc(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SodMCK4RrUY"
      },
      "source": [
        "#Majority Vote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5mxDLJ9Rv3A"
      },
      "outputs": [],
      "source": [
        "X_maj, y_maj = [], []\n",
        "\n",
        "z=0\n",
        "for x in DATA_TEST:\n",
        "        emg = x[\"emg\"]\n",
        "        emg = lpf(emg)\n",
        "        emg = mlaw(emg)\n",
        "        emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "        emg = noise(emg)\n",
        "        if(len(emg)>l):\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "\n",
        "        X_maj.append(emg)\n",
        "        y_maj.append(x)\n",
        "\n",
        "X_maj = np.array(X_maj)\n",
        "y_maj = np.array(y_maj)\n",
        "X_maj.shape, y_maj.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg_XmbKMSPSx"
      },
      "outputs": [],
      "source": [
        "def windowed_array(arr, size=WINDOW_SIZE, slide=WINDOW_SLIDE):\n",
        "    W= len(arr)\n",
        "    windows = []\n",
        "    for i in range(0, W - size, slide):\n",
        "        window = arr[i:i + size]\n",
        "        windows.append(window[:, :, np.newaxis])\n",
        "    return np.stack(windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu8J9WpfR9j9"
      },
      "outputs": [],
      "source": [
        "from keras.utils import Sequence\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from keras.metrics import *\n",
        "from keras.utils import Sequence\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class CustomSequenceGenerator(Sequence):\n",
        "    def __init__(self, data_list, labels, batch_size=4, num_classes=52, shuffle_data=True):\n",
        "        self.data_list = data_list\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle_data = shuffle_data\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data_list) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_x = [windowed_array(self.data_list[i]) for i in batch_indices]\n",
        "        batch_y_dicts = [self.labels[i] for i in batch_indices] # Get the list of dictionaries\n",
        "\n",
        "        # Stack all windows together for training\n",
        "        batch_x = np.concatenate(batch_x, axis=0)\n",
        "\n",
        "        # Extract the 'stimulus' value from each dictionary and repeat for each window\n",
        "        batch_y_labels = [y_dict['stimulus'] for y_dict in batch_y_dicts]\n",
        "        # Adjust labels to be zero-indexed\n",
        "        batch_y_repeated = np.repeat([label - 1 for label in batch_y_labels], batch_x.shape[0] // len(batch_y_labels)) # Repeat label for each window\n",
        "\n",
        "\n",
        "        return batch_x, tf.keras.utils.to_categorical(batch_y_repeated, num_classes=self.num_classes) # Use the extracted labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.data_list))\n",
        "        if self.shuffle_data:\n",
        "            self.indices = shuffle(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocwex_CnSIuS"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_majority_voting(model, X_test, y_test, loss_fn=keras.losses.CategoricalCrossentropy()):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using majority voting on a test set, and also compute loss.\"\"\"\n",
        "\n",
        "    y_pred = []\n",
        "    true_labels_list = []\n",
        "    total_loss = 0\n",
        "    y_prob_avv = []\n",
        "    num_samples = len(X_test)\n",
        "    num_classes = model.output_shape[-1]\n",
        "\n",
        "    # Loop through each test sample\n",
        "    for array, y_dict in zip(X_test, y_test):\n",
        "        # Window the array\n",
        "        windows = windowed_array(array)  # Shape: [num_windows, 15, 10, 1]\n",
        "\n",
        "        # Get model predictions\n",
        "        probs = model.predict(windows, verbose=0)  # Shape: [num_windows, 52]\n",
        "\n",
        "        # Compute per-window predictions (argmax over classes)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "\n",
        "        # Majority voting\n",
        "        voted = mode(preds, keepdims=False).mode\n",
        "        y_pred.append(voted)\n",
        "\n",
        "        # Extract and adjust the true label\n",
        "        true_label = y_dict['stimulus'] - 1 # Adjust label to be zero-indexed\n",
        "        true_labels_list.append(true_label)\n",
        "        avg_prob = np.mean(probs, axis=0)\n",
        "        y_prob_avv.append(avg_prob)\n",
        "        # Calculate loss for the current sample using the true label for its window\n",
        "        true_label_repeated = np.repeat(true_label, windows.shape[0])\n",
        "        # Adjust labels to be zero-indexed before to_categorical\n",
        "        true_label_one_hot = tf.keras.utils.to_categorical(true_label_repeated, num_classes=52)\n",
        "        batch_loss = loss_fn(true_label_one_hot, probs).numpy()  # loss per window\n",
        "        total_loss += np.mean(batch_loss) # Take the mean loss across windows for the sample\n",
        "\n",
        "    # Compute average loss for the entire test set\n",
        "    avg_loss = total_loss / num_samples\n",
        "    true_labels_one_hot = tf.keras.utils.to_categorical(true_labels_list, num_classes=num_classes)\n",
        "    # Compute majority voting accuracy using the adjusted true labels\n",
        "    acc = accuracy_score(true_labels_list, y_pred)\n",
        "    top3acc_tensor = tf.keras.metrics.top_k_categorical_accuracy(true_labels_one_hot, np.array(y_prob_avv), k=3)\n",
        "    top3acc = np.mean(top3acc_tensor.numpy())\n",
        "    top5acc_tensor = tf.keras.metrics.top_k_categorical_accuracy(true_labels_one_hot, np.array(y_prob_avv), k=5)\n",
        "    top5acc = np.mean(top5acc_tensor.numpy())\n",
        "    print(f\"Majority Voting Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Majority Voting Test Top-3 Accuracy: {top3acc:.4f}\")\n",
        "    print(f\"Majority Voting Test Top-5 Accuracy: {top5acc:.4f}\")\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "    return acc, avg_loss, top3acc, top5acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkscSxYTU92L"
      },
      "outputs": [],
      "source": [
        "def predict_with_majority_voting(model, X_data,y_true):\n",
        "    \"\"\"\n",
        "    Predict class for each full array using majority vote over its windows.\n",
        "\n",
        "    \"\"\"\n",
        "    y_pred = []\n",
        "\n",
        "    for array in X_data:\n",
        "        windows = windowed_array(array)  # shape: (num_windows, 15, 10, 1)\n",
        "        probs = model.predict(windows, verbose=0)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        voted = mode(preds, keepdims=False).mode\n",
        "        y_pred.append(voted)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Predict Majority Voting Accuracy: {acc:.4f}\")\n",
        "\n",
        "    return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED-YnBwCrw97"
      },
      "source": [
        "#ATZORINET WITH RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndd4vLURrc0F"
      },
      "source": [
        "#ATZORINET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqeRai6-hDN0"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import *\n",
        "from keras import initializers, regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def Atzorinet(input_shape, classes = 52, l2 = 0.0005, dropout = 0.2, batch_norm = True):\n",
        "    #creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    X = ZeroPadding2D((0, 4))(X)\n",
        "    X = Conv2D(32, (1, 10), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block1_conv2d_32_1_10')(X)\n",
        "    X = Activation('relu', name='block1_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block1_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = Conv2D(32, (3, 3), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block2_conv2d_32_3_3')(X)\n",
        "    X = Activation('relu', name='block2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block2_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block2_average_pool')(X)\n",
        "\n",
        "    # Block 3\n",
        "    X = ZeroPadding2D((2, 2))(X)\n",
        "    X = Conv2D(64, (5, 5), padding='valid',#kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block3_conv2d_64_5_5')(X)\n",
        "    X = Activation('relu', name='block3_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block3_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block3_pool')(X)\n",
        "\n",
        "    # Block 4\n",
        "    X = ZeroPadding2D((2, 0))(X)\n",
        "    X = Conv2D(64, (5, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block4_conv2d_64_5x1')(X)\n",
        "    X = Activation('relu', name='block4_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block4_dropout')(X)\n",
        "\n",
        "    # Block 5\n",
        "    X = Conv2D(classes, (1, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block5_conv2d_{}_1x1'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block5_soft')(X)\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = Atzorinet((15,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iTo1Un5rskT"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras import initializers, regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def AtzorinetRNN_Bid(input_shape, classes = 52, l2 = 0.0005, dropout = 0.2, batch_norm = True):\n",
        "    #creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    X = ZeroPadding2D((0, 4))(X)\n",
        "    X = Conv2D(32, (1, 10), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block1_conv2d_32_1_10')(X)\n",
        "    X = Activation('relu', name='block1_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block1_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = Conv2D(32, (3, 3), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block2_conv2d_32_3_3')(X)\n",
        "    X = Activation('relu', name='block2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block2_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block2_average_pool')(X)\n",
        "\n",
        "    # Block 3\n",
        "    X = ZeroPadding2D((2, 2))(X)\n",
        "    X = Conv2D(64, (5, 5), padding='valid',#kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block3_conv2d_64_5_5')(X)\n",
        "    X = Activation('relu', name='block3_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block3_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block3_pool')(X)\n",
        "\n",
        "    # Block 4\n",
        "    X = ZeroPadding2D((2, 0))(X)\n",
        "    X = Conv2D(64, (5, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block4_conv2d_64_5x1')(X)\n",
        "    X = Activation('relu', name='block4_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block4_dropout')(X)\n",
        "\n",
        "    # Block 5\n",
        "    time_steps = X.shape[1] * X.shape[2]  # Combine spatial dimensions\n",
        "    features = X.shape[3]  # Channels\n",
        "    X = Reshape((time_steps, features))(X)\n",
        "    X = Reshape((2,32))(X)\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul, name='rnn_lstm')\n",
        "    X = Bidirectional(lstm_layer)(X)\n",
        "    X = Activation('relu', name='block5_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block5_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul, name='rnn_lstm_2')\n",
        "    X = Bidirectional(lstm_layer_2)(X)\n",
        "    X = Activation('relu', name='block5_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block5_2_dropout')(X)\n",
        "\n",
        "    # Block 6\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block6_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block6_soft')(X)\n",
        "    # Create Model\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_with_RNN')\n",
        "\n",
        "    return model\n",
        "model1 = AtzorinetRNN_Bid((15,10,1))\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HBUAN3SLO5Zc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras import initializers, regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def AtzorinetRNN(input_shape, classes = 52, l2 = 0.0005, dropout = 0.2, batch_norm = True):\n",
        "    #creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    X = ZeroPadding2D((0, 4))(X)\n",
        "    X = Conv2D(32, (1, 10), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block1_conv2d_32_1_10')(X)\n",
        "    X = Activation('relu', name='block1_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block1_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = Conv2D(32, (3, 3), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block2_conv2d_32_3_3')(X)\n",
        "    X = Activation('relu', name='block2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block2_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block2_average_pool')(X)\n",
        "\n",
        "    # Block 3\n",
        "    X = ZeroPadding2D((2, 2))(X)\n",
        "    X = Conv2D(64, (5, 5), padding='valid',#kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block3_conv2d_64_5_5')(X)\n",
        "    X = Activation('relu', name='block3_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block3_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block3_pool')(X)\n",
        "\n",
        "    # Block 4\n",
        "    X = ZeroPadding2D((2, 0))(X)\n",
        "    X = Conv2D(64, (5, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block4_conv2d_64_5x1')(X)\n",
        "    X = Activation('relu', name='block4_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block4_dropout')(X)\n",
        "\n",
        "    # Block 5\n",
        "    time_steps = X.shape[1] * X.shape[2]  # Combine spatial dimensions\n",
        "    features = X.shape[3]  # Channels\n",
        "    X = Reshape((time_steps, features))(X)\n",
        "    X = Reshape((2,32))(X)\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul, name='rnn_lstm')\n",
        "    X = lstm_layer(X)\n",
        "    X = Activation('relu', name='block5_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block5_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul, name='rnn_lstm_2')\n",
        "    X = lstm_layer_2(X)\n",
        "    X = Activation('relu', name='block5_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block5_2_dropout')(X)\n",
        "\n",
        "    # Block 6\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block6_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block6_soft')(X)\n",
        "    # Create Model\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_with_RNN')\n",
        "\n",
        "    return model\n",
        "model1 = AtzorinetRNN((15,10,1))\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvtPiEb1g0uU"
      },
      "source": [
        "#Time Distributed with RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FFrQTEQmazxW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras import initializers, regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "def Atzorinet2(input_shape=(5, 10, 1), classes=52, l2=0.0005, dropout=0.2, batch_norm=True):\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    X_input = Input(input_shape, name='block0_input')\n",
        "    X = X_input\n",
        "\n",
        "    # Block 1: Conv on full width\n",
        "    X = Conv2D(32, (1, 10), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    if batch_norm: X = BatchNormalization()(X)\n",
        "    if dropout > 0: X = Dropout(dropout)(X)\n",
        "\n",
        "    # Block 2: 3x3 conv, with SAME padding\n",
        "    X = Conv2D(32, (3, 3), padding='same', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    if batch_norm: X = BatchNormalization()(X)\n",
        "    if dropout > 0: X = Dropout(dropout)(X)\n",
        "    # Removed pooling to preserve size\n",
        "\n",
        "    # Block 3: 3x3 conv, same padding\n",
        "    X = Conv2D(64, (3, 3), padding='same', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    if batch_norm: X = BatchNormalization()(X)\n",
        "    if dropout > 0: X = Dropout(dropout)(X)\n",
        "    # Removed pooling again to preserve (5,1) height\n",
        "\n",
        "    # Block 4: Final Conv (5x1) â€” keep this\n",
        "    X = Conv2D(64, (5, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    # Output: flatten\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    model = Model(inputs=X_input, outputs=X, name='Atzori2')\n",
        "    return model\n",
        "model = Atzorinet2((5,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tal7IN14g3uA"
      },
      "outputs": [],
      "source": [
        "def Atzorinet_Time_Bidir(input_shape, classes=52, l2=0.0005, dropout=0.2, batch_norm=True):\n",
        "    # creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    atzorinet_layer = Atzorinet2((5, 10, 1))\n",
        "    X = Reshape((3, 5, 10, 1))(X)\n",
        "    X = TimeDistributed(atzorinet_layer)(X)\n",
        "\n",
        "    # Block LSTM\n",
        "    # Reshape to combine spatial dimensions and maintain 3D input for LSTM\n",
        "    shape_x = X.shape\n",
        "    # The output of TimeDistributed is (None, 3, 1, 1, 64).\n",
        "    # We want to reshape to (None, 3, 64) for the LSTM.\n",
        "    #X = Reshape((shape_x[1], shape_x[4]))(X)  # Reshape to (None, 3, 64)\n",
        "    X = Reshape ((6,32))(X)\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                      name='rnn_lstm')\n",
        "    X = Bidirectional(lstm_layer)(X)\n",
        "    X = Activation('relu', name='LSTM_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                        name='rnn_lstm_2')\n",
        "    X = Bidirectional(lstm_layer_2)(X)\n",
        "    X = Activation('relu', name='LSTM_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_2_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "              name='block2_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block2_soft')(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_Time_Bidir')\n",
        "    return model\n",
        "model = Atzorinet_Time_Bidir((15,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TH6BHZENg5z6"
      },
      "outputs": [],
      "source": [
        "def Atzorinet_Time(input_shape, classes=52, l2=0.0005, dropout=0.2, batch_norm=True):\n",
        "    # creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    atzorinet_layer = Atzorinet2((5, 10, 1))\n",
        "    X = Reshape((3, 5, 10, 1))(X)\n",
        "    X = TimeDistributed(atzorinet_layer)(X)\n",
        "\n",
        "    # Block LSTM\n",
        "    # Reshape to combine spatial dimensions and maintain 3D input for LSTM\n",
        "    shape_x = X.shape\n",
        "    X = Reshape ((6,32))(X)\n",
        "    # The output of TimeDistributed is (None, 3, 1, 1, 64).\n",
        "    # We want to reshape to (None, 3, 64) for the LSTM.\n",
        "    #X = Reshape((shape_x[1], shape_x[4]))(X)  # Reshape to (None, 3, 64)\n",
        "\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                      name='rnn_lstm')\n",
        "    X = lstm_layer(X)\n",
        "    X = Activation('relu', name='LSTM_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                        name='rnn_lstm_2')\n",
        "    X = lstm_layer_2(X)\n",
        "    X = Activation('relu', name='LSTM_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_2_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "              name='block2_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block2_soft')(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_Time')\n",
        "    return model\n",
        "model = Atzorinet_Time((15,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWHMrpuiedl0"
      },
      "source": [
        "#Time Distributed RNN with 45 window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6_ZEB5EKFDhS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.layers import *\n",
        "from keras import initializers, regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def Atzorinet3(input_shape, classes = 52, l2 = 0.0005, dropout = 0.2, batch_norm = True):\n",
        "    #creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    X = ZeroPadding2D((0, 4))(X)\n",
        "    X = Conv2D(32, (1, 10), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block1_conv2d_32_1_10')(X)\n",
        "    X = Activation('relu', name='block1_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block1_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = Conv2D(32, (3, 3), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block2_conv2d_32_3_3')(X)\n",
        "    X = Activation('relu', name='block2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block2_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block2_average_pool')(X)\n",
        "\n",
        "    # Block 3\n",
        "    X = ZeroPadding2D((2, 2))(X)\n",
        "    X = Conv2D(64, (5, 5), padding='valid',#kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block3_conv2d_64_5_5')(X)\n",
        "    X = Activation('relu', name='block3_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='block3_dropout')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(3, 3), name='block3_pool')(X)\n",
        "\n",
        "    # Block 4\n",
        "    X = ZeroPadding2D((2, 0))(X)\n",
        "    X = Conv2D(64, (5, 1), padding='valid', kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "               name='block4_conv2d_64_5x1')(X)\n",
        "    X = Activation('relu', name='block4_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = Atzorinet3((15,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY6UDSUvecOh"
      },
      "outputs": [],
      "source": [
        "def Atzorinet_Time_Bidir2(input_shape, classes=52, l2=0.0005, dropout=0.2, batch_norm=True):\n",
        "    # creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    atzorinet_layer = Atzorinet3((15, 10, 1))\n",
        "    X = Reshape((3, 15, 10, 1))(X)\n",
        "    X = TimeDistributed(atzorinet_layer)(X)\n",
        "\n",
        "    # Block LSTM\n",
        "    # Reshape to combine spatial dimensions and maintain 3D input for LSTM\n",
        "    shape_x = X.shape\n",
        "    # The output of TimeDistributed is (None, 3, 1, 1, 64).\n",
        "    # We want to reshape to (None, 3, 64) for the LSTM.\n",
        "    #X = Reshape((shape_x[1], shape_x[4]))(X)  # Reshape to (None, 3, 64)\n",
        "    X = Reshape ((6,32))(X)\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                      name='rnn_lstm')\n",
        "    X = Bidirectional(lstm_layer)(X)\n",
        "    X = Activation('relu', name='LSTM_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                        name='rnn_lstm_2')\n",
        "    X = Bidirectional(lstm_layer_2)(X)\n",
        "    X = Activation('relu', name='LSTM_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_2_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "              name='block2_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block2_soft')(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_Time_Bidir')\n",
        "    return model\n",
        "model = Atzorinet_Time_Bidir2((45,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-y-YeFecfY"
      },
      "outputs": [],
      "source": [
        "def Atzorinet_Time2(input_shape, classes=52, l2=0.0005, dropout=0.2, batch_norm=True):\n",
        "    # creating the CNN\n",
        "    kernel_regul = regularizers.l2(l2)\n",
        "    kernel_init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    # Block 0\n",
        "    input = Input(input_shape, name='block0_input')\n",
        "    X = input\n",
        "\n",
        "    # Block 1\n",
        "    atzorinet_layer = Atzorinet3((15, 10, 1))\n",
        "    X = Reshape((3, 15, 10, 1))(X)\n",
        "    X = TimeDistributed(atzorinet_layer)(X)\n",
        "\n",
        "    # Block LSTM\n",
        "    # Reshape to combine spatial dimensions and maintain 3D input for LSTM\n",
        "    shape_x = X.shape\n",
        "    # The output of TimeDistributed is (None, 3, 1, 1, 64).\n",
        "    # We want to reshape to (None, 3, 64) for the LSTM.\n",
        "    #X = Reshape((shape_x[1], shape_x[4]))(X)  # Reshape to (None, 3, 64)\n",
        "    X = Reshape ((6,32))(X)\n",
        "    lstm_layer = LSTM(128, return_sequences=True, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                      name='rnn_lstm')\n",
        "    X = lstm_layer(X)\n",
        "    X = Activation('relu', name='LSTM_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_dropout')(X)\n",
        "    lstm_layer_2 = LSTM(64, return_sequences=False, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "                        name='rnn_lstm_2')\n",
        "    X = lstm_layer_2(X)\n",
        "    X = Activation('relu', name='LSTM_2_relu')(X)\n",
        "    if batch_norm:\n",
        "        X = BatchNormalization()(X)\n",
        "    if dropout > 0:\n",
        "        X = Dropout(dropout, name='LSTM_2_dropout')(X)\n",
        "\n",
        "    # Block 2\n",
        "    X = Dense(classes, kernel_initializer=kernel_init, kernel_regularizer=kernel_regul,\n",
        "              name='block2_Dense'.format(classes))(X)\n",
        "    X = Activation('softmax', name='block2_soft')(X)\n",
        "\n",
        "    model = Model(inputs=input, outputs=X, name='Atzori_Time')\n",
        "    return model\n",
        "model = Atzorinet_Time2((45,10,1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m1C6jHklglI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mqrZc8fQshd"
      },
      "source": [
        "#**Test**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RsLcFv0WQqw"
      },
      "source": [
        "#AtzoriNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Oa4ZbSayhJSM"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "import os\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
        "                                            save_best_only=True)\n",
        "\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[2]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = Atzorinet((15,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for Atzorinet with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"Atzorinet with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "#clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI7QYZEWW8RC"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ahPQcKaxW-ea"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[2]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = AtzorinetRNN((15,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for AtzorinetRNN with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"AtzorinetRNN with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/AtzorinetRNN/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/AtzorinetRNN/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E-jpBKmXNKE"
      },
      "source": [
        "#RNN BIDIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJtsmwwXP4j"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[2]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = AtzorinetRNN_Bid((15,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for AtzorinetRNN with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"AtzorinetRNN with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/AtzorinetRNN_Bid/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/AtzorinetRNN_Bid/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "#clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bzj8PYKXv3o"
      },
      "source": [
        "#TIME RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7Bl5mMhdYY3i"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[2]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = Atzorinet_Time((15,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for Atzorinet_Time with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"Atzorinet_Time with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVttU4iwbmpl"
      },
      "source": [
        "#Time Bid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuOLONEukWtI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[2]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = Atzorinet_Time_Bidir((15,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for Atzorinet_Time with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"Atzorinet_Time with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time_Bid/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time_Bid/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227HA4nRXoVI"
      },
      "source": [
        "#Preprocessing for 45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ApZklTXoVI"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 45\n",
        "WINDOW_SLIDE = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAI_XhUXXoVI"
      },
      "outputs": [],
      "source": [
        "import scipy.signal\n",
        "import math\n",
        "def lpf(x):\n",
        "    f = 1 / 50\n",
        "    b, a = scipy.signal.butter(1, f, 'low')\n",
        "    output = scipy.signal.filtfilt(b, a, x, axis=0)\n",
        "    return output\n",
        "\n",
        "def noise(x, snr_l=25):\n",
        "    snr_u = 45\n",
        "    db = np.random.randint(snr_l, snr_u, (1,))[0]\n",
        "    snr = 10 ** (db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0) / x.shape[0]\n",
        "    Np = Xp / snr\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np))\n",
        "    xn = x + n\n",
        "    return xn\n",
        "\n",
        "def mlaw(x):\n",
        "  x = (x - np.min(x)) / (np.max(x) - np.min(x))*2 - 1\n",
        "  m = 32\n",
        "  # Apply the mlaw logic element-wise\n",
        "  mlaw_func = np.vectorize(lambda val: (math.log(1 + m * abs(val)) / math.log(1 + m)) * math.copysign(1, val))\n",
        "  return mlaw_func(x)\n",
        "\n",
        "l=180\n",
        "for x in DATA_TRAIN:\n",
        "    emg = x[\"emg\"]\n",
        "    if l > len(emg):\n",
        "        l = len(emg)\n",
        "for x in DATA_TEST:\n",
        "    emg = x[\"emg\"]\n",
        "    if l > len(emg):\n",
        "        l = len(emg)\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XnSvyuXWXoVJ"
      },
      "outputs": [],
      "source": [
        "emg = DATA_TRAIN[0][\"emg\"]\n",
        "plt.plot(emg)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5a_zGvCqXoVJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "emg = np.array(emg)\n",
        "print(emg.shape)\n",
        "cut = int((len(emg)-l)/2)\n",
        "for i in range(cut):\n",
        "    emg=np.delete(emg, 0, 0)\n",
        "for i in range (len(emg)-l):\n",
        "    emg=np.delete(emg, l, 0)\n",
        "plt.plot(emg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jdPbDAyrXoVJ"
      },
      "outputs": [],
      "source": [
        "emg=lpf(emg)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WaN_2YkXXoVJ"
      },
      "outputs": [],
      "source": [
        "emg=mlaw(emg)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NaqPshXHXoVJ"
      },
      "outputs": [],
      "source": [
        "plt.plot((emg - np.min(emg)) / (np.max(emg) - np.min(emg)))\n",
        "emg= (emg - np.min(emg)) / (np.max(emg) - np.min(emg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1DG4qxlfXoVJ"
      },
      "outputs": [],
      "source": [
        "emg = noise(emg)\n",
        "print (emg.shape)\n",
        "plt.plot(emg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doxx8v5gXoVK"
      },
      "outputs": [],
      "source": [
        "import scipy.signal\n",
        "X_train, y_train = [], []\n",
        "def lpf(x):\n",
        "    f = 1 / 50\n",
        "    b, a = scipy.signal.butter(1, f, 'low')\n",
        "    output = scipy.signal.filtfilt(b, a, x, axis=0)\n",
        "    return output\n",
        "\n",
        "def noise(x, snr_l=25):\n",
        "    snr_u = 45\n",
        "    db = np.random.randint(snr_l, snr_u, (1,))[0]\n",
        "    snr = 10 ** (db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0) / x.shape[0]\n",
        "    Np = Xp / snr\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np))\n",
        "    xn = x + n\n",
        "    return xn\n",
        "\n",
        "def mlaw(x):\n",
        "  m = 32\n",
        "  if m==0: return x\n",
        "\n",
        "\n",
        "  #x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "  # Apply the mlaw logic element-wise\n",
        "  mlaw_func = np.vectorize(lambda val: (math.log(1 + m * abs(val)) / math.log(1 + m)) * math.copysign(1, val))\n",
        "  return mlaw_func(x)\n",
        "\n",
        "\n",
        "aug = 4\n",
        "\n",
        "z=0\n",
        "for j in range (aug):\n",
        "    for x in DATA_TRAIN:\n",
        "\n",
        "        emg = x[\"emg\"]\n",
        "        emg = lpf(emg)\n",
        "        emg = mlaw(emg)\n",
        "        emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "        emg = noise(emg)\n",
        "        if(len(emg)>l):\n",
        "          z=z+1\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "        for i in range(0, len(emg)-WINDOW_SIZE, WINDOW_SLIDE):\n",
        "            X_train.append(emg[i:i+WINDOW_SIZE, :])\n",
        "            y_train.append(x[\"stimulus\"])\n",
        "print(z)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S_BPgDhYXoVK"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = [], []\n",
        "\n",
        "for x in DATA_TEST:\n",
        "    emg = x[\"emg\"]\n",
        "    emg = lpf(emg)\n",
        "    emg = mlaw(emg)\n",
        "    emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "    emg = noise(emg)\n",
        "    if(len(emg)>l):\n",
        "          z=z+1\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "    for i in range(0, len(emg)-WINDOW_SIZE, WINDOW_SLIDE):\n",
        "        X_test.append(emg[i:i+WINDOW_SIZE, :])\n",
        "        y_test.append(x[\"stimulus\"])\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dnp4eg_AXoVK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(np.unique(y_train))\n",
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9sLccmvXoVK"
      },
      "outputs": [],
      "source": [
        "!source /content/drive/MyDrive/Diplo2/semgs/bin/activate\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Labels 1-52 should be mapped to 0-51 and then encoded\n",
        "y_train = to_categorical(le.transform(y_train), 52)\n",
        "y_test = to_categorical(le.transform(y_test), 52)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGkgDkdrXoVK"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18KtQ8bBXoVK"
      },
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuxDPlyrXoVK"
      },
      "outputs": [],
      "source": [
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IpojvFIqXoVK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "indices = np.random.choice(len(X_train), len(X_train), False)\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "indices = np.random.choice(len(X_test), len(X_test), False)\n",
        "X_test = X_test[indices]\n",
        "y_test = y_test[indices]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwoqNnrpYTFV"
      },
      "outputs": [],
      "source": [
        "X_maj, y_maj = [], []\n",
        "\n",
        "z=0\n",
        "for x in DATA_TEST:\n",
        "        emg = x[\"emg\"]\n",
        "        emg = lpf(emg)\n",
        "        emg = mlaw(emg)\n",
        "        emg = (emg - np.min(emg)) / (np.max(emg) - np.min(emg))\n",
        "        emg = noise(emg)\n",
        "        if(len(emg)>l):\n",
        "          cut = int((len(emg)-l)/2)\n",
        "          for i in range(cut):\n",
        "            emg=np.delete(emg, 0, 0)\n",
        "          for i in range (len(emg)-l):\n",
        "            emg=np.delete(emg, l, 0)\n",
        "\n",
        "        X_maj.append(emg)\n",
        "        y_maj.append(x)\n",
        "\n",
        "X_maj = np.array(X_maj)\n",
        "y_maj = np.array(y_maj)\n",
        "X_maj.shape, y_maj.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8GUSgIcYTFV"
      },
      "outputs": [],
      "source": [
        "def windowed_array(arr, size=WINDOW_SIZE, slide=WINDOW_SLIDE):\n",
        "    W= len(arr)\n",
        "    windows = []\n",
        "    for i in range(0, W - size, slide):\n",
        "        window = arr[i:i + size]\n",
        "        windows.append(window[:, :, np.newaxis])\n",
        "    return np.stack(windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2w70KkWYTFW"
      },
      "outputs": [],
      "source": [
        "from keras.utils import Sequence\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from keras.metrics import *\n",
        "from keras.utils import Sequence\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class CustomSequenceGenerator(Sequence):\n",
        "    def __init__(self, data_list, labels, batch_size=4, num_classes=52, shuffle_data=True):\n",
        "        self.data_list = data_list\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle_data = shuffle_data\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data_list) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_x = [windowed_array(self.data_list[i]) for i in batch_indices]\n",
        "        batch_y_dicts = [self.labels[i] for i in batch_indices] # Get the list of dictionaries\n",
        "\n",
        "        # Stack all windows together for training\n",
        "        batch_x = np.concatenate(batch_x, axis=0)\n",
        "\n",
        "        # Extract the 'stimulus' value from each dictionary and repeat for each window\n",
        "        batch_y_labels = [y_dict['stimulus'] for y_dict in batch_y_dicts]\n",
        "        # Adjust labels to be zero-indexed\n",
        "        batch_y_repeated = np.repeat([label - 1 for label in batch_y_labels], batch_x.shape[0] // len(batch_y_labels)) # Repeat label for each window\n",
        "\n",
        "\n",
        "        return batch_x, tf.keras.utils.to_categorical(batch_y_repeated, num_classes=self.num_classes) # Use the extracted labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.data_list))\n",
        "        if self.shuffle_data:\n",
        "            self.indices = shuffle(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S41yFERGYTFW"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_majority_voting(model, X_test, y_test, loss_fn=keras.losses.CategoricalCrossentropy()):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using majority voting on a test set, and also compute loss.\"\"\"\n",
        "\n",
        "    y_pred = []\n",
        "    true_labels_list = []\n",
        "    total_loss = 0\n",
        "    y_prob_avv = []\n",
        "    num_samples = len(X_test)\n",
        "    num_classes = model.output_shape[-1]\n",
        "\n",
        "    # Loop through each test sample\n",
        "    for array, y_dict in zip(X_test, y_test):\n",
        "        # Window the array\n",
        "        windows = windowed_array(array)  # Shape: [num_windows, 15, 10, 1]\n",
        "\n",
        "        # Get model predictions\n",
        "        probs = model.predict(windows, verbose=0)  # Shape: [num_windows, 52]\n",
        "\n",
        "        # Compute per-window predictions (argmax over classes)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "\n",
        "        # Majority voting\n",
        "        voted = mode(preds, keepdims=False).mode\n",
        "        y_pred.append(voted)\n",
        "\n",
        "        # Extract and adjust the true label\n",
        "        true_label = y_dict['stimulus'] - 1 # Adjust label to be zero-indexed\n",
        "        true_labels_list.append(true_label)\n",
        "\n",
        "        avg_prob = np.mean(probs, axis=0)\n",
        "        y_prob_avv.append(avg_prob)\n",
        "        # Calculate loss for the current sample using the true label for its window\n",
        "        true_label_repeated = np.repeat(true_label, windows.shape[0])\n",
        "        true_label_one_hot = tf.keras.utils.to_categorical(true_label_repeated, num_classes=52)\n",
        "        batch_loss = loss_fn(true_label_one_hot, probs).numpy()  # loss per window\n",
        "        total_loss += np.mean(batch_loss) # Take the mean loss across windows for the sample\n",
        "\n",
        "    # Compute average loss for the entire test set\n",
        "    avg_loss = total_loss / num_samples\n",
        "    true_labels_one_hot = tf.keras.utils.to_categorical(true_labels_list, num_classes=num_classes)\n",
        "    # Compute majority voting accuracy using the adjusted true labels\n",
        "    acc = accuracy_score(true_labels_list, y_pred)\n",
        "    top3acc_tensor = tf.keras.metrics.top_k_categorical_accuracy(true_labels_one_hot, np.array(y_prob_avv), k=3)\n",
        "    top3acc = np.mean(top3acc_tensor.numpy())\n",
        "    top5acc_tensor = tf.keras.metrics.top_k_categorical_accuracy(true_labels_one_hot, np.array(y_prob_avv), k=5)\n",
        "    top5acc = np.mean(top5acc_tensor.numpy())\n",
        "    print(f\"Majority Voting Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Majority Voting Test Top-3 Accuracy: {top3acc:.4f}\")\n",
        "    print(f\"Majority Voting Test Top-5 Accuracy: {top5acc:.4f}\")\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "    return acc, avg_loss, top3acc, top5acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ3hs9CBYTFW"
      },
      "outputs": [],
      "source": [
        "def predict_with_majority_voting(model, X_data,y_true):\n",
        "    \"\"\"\n",
        "    Predict class for each full array using majority vote over its windows.\n",
        "\n",
        "    \"\"\"\n",
        "    y_pred = []\n",
        "\n",
        "    for array in X_data:\n",
        "        windows = windowed_array(array)  # shape: (num_windows, 15, 10, 1)\n",
        "        probs = model.predict(windows, verbose=0)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        voted = mode(preds, keepdims=False).mode\n",
        "        y_pred.append(voted)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Predict Majority Voting Accuracy: {acc:.4f}\")\n",
        "\n",
        "    return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw9KFEP8bhn0"
      },
      "source": [
        "#Time 45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je9cQdLIbqm1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time_45/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time_45/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[0]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = Atzorinet_Time2((45,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for Atzorinet_Tim_45 with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"Atzorinet_Tim_45 with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time_45/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time_45/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "clear_session()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DzLqfLR3pq"
      },
      "source": [
        "#Time_45 Bidir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HQ8N60cR7_x"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.backend import clear_session\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time_Bidir45/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "file = \"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time_Bidir45/\"\n",
        "if not os.path.exists(file):\n",
        "    os.makedirs(file)\n",
        "\n",
        "\n",
        "keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\", min_delta=0.001)\n",
        "checkpoint_filepath=\"/content/drive/MyDrive/Diplo2/first_model.keras\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max',\n",
        "                                            save_best_only=True)\n",
        "optimizer = optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    loss_scale_factor=None,\n",
        "    gradient_accumulation_steps=None,\n",
        "    name='adam',\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = [64,128,256,512]\n",
        "epochs = [10,20,30,50]\n",
        "batch_size = batch_size[0]\n",
        "epochs = epochs[3]\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "model = Atzorinet_Time_Bidir2((45,10,1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', top3acc, top5acc])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback,model_checkpoint_callback], shuffle = True)\n",
        "print(\"Evaluate on test data for Atzorinet_Tim_45 with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Diplo2/first_model.keras\",custom_objects={'top3acc': top3acc, 'top5acc': top5acc})\n",
        "results = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print(\"Atzorinet_Tim_45 with batch={} and epochs={}\".format(batch_size, epochs))\n",
        "print(\"test loss, test ac, top3, top5:\", results)\n",
        "y_prediction = model.predict(X_test)\n",
        "y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "y_test_original = y_test\n",
        "y_test=np.argmax(y_test, axis = 1)\n",
        "#Create confusion matrix and normalizes it over predicted (columns)\n",
        "cm = confusion_matrix(y_test, y_prediction)\n",
        "cm=np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion/Atzorinet_Time_Bidir45/{}\".format(s),cm)\n",
        "y_test = y_test_original\n",
        "evaluate_with_majority_voting(model, X_maj, y_maj)\n",
        "y_true = np.array([y_dict['stimulus'] - 1 for y_dict in y_maj])\n",
        "y_val_pred = predict_with_majority_voting(model, X_maj, y_true)\n",
        "cm = confusion_matrix(y_true, y_val_pred)\n",
        "cm = np.array(cm)\n",
        "np.save(\"/content/drive/MyDrive/Diplo2/Confusion_Majority/Atzorinet_Time_Bidir45/{}\".format(s),cm)\n",
        "\n",
        "del model\n",
        "\n",
        "clear_session()\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4PnmiUR1P0AN",
        "CpJ_N9bwP7B-",
        "HMRu7WS2QEjC",
        "zAS8jKy6QHOf",
        "4SodMCK4RrUY",
        "ndd4vLURrc0F",
        "PvtPiEb1g0uU",
        "KWHMrpuiedl0",
        "7RsLcFv0WQqw",
        "dI7QYZEWW8RC",
        "6E-jpBKmXNKE",
        "2Bzj8PYKXv3o",
        "cVttU4iwbmpl",
        "227HA4nRXoVI",
        "Lw9KFEP8bhn0",
        "s_DzLqfLR3pq"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}